{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am doing semnatic analysis for facebook comments on different posts. I am using a classic machine learning approach. \n",
    "\n",
    "I wont put data public here, but if you have your own facebook page (you are the admin of a page) you can download the comments from the posts and run this code for your own posts. I am randomly taking comments from different posts and do the training on them. The results are not super good yet as I need much higher numbrs of labels. \n",
    "\n",
    "\n",
    "Here, I explain how we make words into vectors (text vectorization). One method to use is called CountVectorizer and you can have it imported from python: \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "Tokenization and filtering stopworks are all included in this library (so no need to do use many parts of previous cell!). This method counts the number of times a word appears in the document and make a sparse matrix relatively. This is exactly what bag of word (BOW) will do for us!  \n",
    "I use the example from https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html python documentary:\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "1) For the corpus awe distinguish all the used words which are: \n",
    "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "\n",
    "2) For each sentence we see if the word is appeared, in this case for \"This is the first document\" we will have:\n",
    "[0, 1, 1, 1, 0, 0, 1, 0, 1] \n",
    "This means that in the first sentence \"and\" has not been used. We can go ahead and make a matrix represnting the corpus. The commands can be found in the above documnet.\n",
    "\n",
    "Remeber, in BOW we loose the word order and the counts are not normalized. However, we can count token paris (n-grams) which CountVectorizer in Python is capable of doing it. \n",
    "\n",
    "Another usefull thing we use is called TF/IDF:\n",
    "\n",
    "I explain TF first: Term Frequency tf(t,d) is the frequency for term (or n-grams) in a documemnt. We can calculate it as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "tf(t,d) = \\frac {f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}\n",
    "\\end{equation*}\n",
    "\n",
    "where $f_{t,d}$ is the raw counts of the term and $\\sum_{t'} f_{t',d}$ is the summation of all terms; so the TF gives a probability of a term being used. We can also have the log norm of the term frquency:\n",
    "\n",
    "\\begin{equation*}\n",
    "tf(t,d) =  1 + \\log(f_{t,d})\n",
    "\\end{equation*}\n",
    "\n",
    "Now I explain Inverese Document Frequency (IDF):\n",
    "\n",
    "We define $N = D$ as total number of documents in a corpus and $\\mid d \\in D: t \\in d \\mid$ as number of documents when terms t appears. And, we can write:\n",
    "\\begin{equation*}\n",
    "idf(t,D) =  \\log \\frac{N}{\\mid d \\in D: t \\in d \\mid}\n",
    "\\end{equation*}\n",
    "\n",
    "So, TF-IDF can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "tfidf(t,d,D) =  tf(t,d).idf(t, D)\n",
    "\\end{equation*}\n",
    "\n",
    "Now, this is obvious that why a high weight TF-IDF is desirable: because it means that it has high term frequency in a document, but low frequency in the WHOLE collection of documnets in the corpus. \n",
    "To make TFIDF for tokens (n_grams) in our corpus we can use the following library:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "So, when we have our text we need to use both vectorizer and tfidf transformer. And, then our data is ready to be trained. In order to make 1) vectorizer 2) transformer 3) classifier we can use Pipeline calss in scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells are about loading my comments which I cannot share with you :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "March03 = pd.read_csv(\"labels_March03.csv\")\n",
    "March12 = pd.read_csv(\"labels_March12.csv\")\n",
    "March13 = pd.read_csv(\"labels_March13.csv\")\n",
    "March16 = pd.read_csv(\"labels_March16.csv\")\n",
    "March27 = pd.read_csv(\"labels_March27_2nd.csv\")\n",
    "April9 = pd.read_csv(\"labels_April9.csv\")\n",
    "April13 = pd.read_csv(\"labels_April13.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = pd.concat([March03, March12, March13, March16, March27, April9, April13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only look at the comments with labels with high confidene in them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = Labels[[\"comment\",\"label\", \"conffidence\"]]\n",
    "Labels = Labels[Labels.conffidence>.7]\n",
    "Labels = Labels.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment        4348\n",
       "label          4348\n",
       "conffidence    4348\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = Labels[Labels.label != \"I cannot tell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment        3842\n",
       "label          3842\n",
       "conffidence    3842\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels['label'] = Labels['label'].replace(\"disgusted\", \"angry\")\n",
    "Labels['label'] = Labels['label'].replace(\"angry/disgusted\", \"angry\")\n",
    "Labels['label'] = Labels['label'].replace(\"worried/scared\", \"worried\")\n",
    "Labels['label'] = Labels['label'].replace(\"sad\", \"worried\")\n",
    "Labels['label'] = Labels['label'].replace(\"scared\", \"worried\")\n",
    "Labels['label'] = Labels['label'].replace(\"indifferent\", \"neutral\")\n",
    "Labels['label'] = Labels['label'].replace(\"happy\", \"positive\")\n",
    "Labels['label'] = Labels['label'].replace(\"supportive\", \"positive\")\n",
    "Labels['label'] = Labels['label'].replace(\"happy/supportive/positive\", \"positive\")\n",
    "Labels['label'] = Labels['label'].replace(\"worried/scared/sad\", \"worried\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels['label'] = Labels['label'].replace(\"angry\", \"negative\")\n",
    "# Labels['label'] = Labels['label'].replace(\"worried\", \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_count = Labels[Labels.label == 'neutral'].count()\n",
    "positive_count = Labels[Labels.label == 'positive'].count()\n",
    "worried_count = Labels[Labels.label == 'worried'].count()\n",
    "angry_count = Labels[Labels.label == 'angry'].count()\n",
    "#positive_count = Labels[Labels.label == 'positive'].count()\n",
    "#negative_count = Labels[Labels.label == 'negative'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry_count': comment        1772\n",
       " label          1772\n",
       " conffidence    1772\n",
       " dtype: int64, 'neutral_count': comment        856\n",
       " label          856\n",
       " conffidence    856\n",
       " dtype: int64, 'positive_count': comment        796\n",
       " label          796\n",
       " conffidence    796\n",
       " dtype: int64, 'worried_count': comment        417\n",
       " label          417\n",
       " conffidence    417\n",
       " dtype: int64}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'neutral_count': neutral_count, 'positive_count': positive_count, 'worried_count':worried_count, 'angry_count': angry_count , 'positive_count':positive_count }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment        2870\n",
       "label          2870\n",
       "conffidence    2870\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# many people are angry! so, I am down sampling the angry comments!\n",
    "angry = Labels[Labels.label == \"angry\"].sample(n=800, frac=None)\n",
    "no_angry = Labels[Labels.label != \"angry\"]\n",
    "Labels = pd.concat([angry, no_angry], axis = 0)\n",
    "Labels = Labels.sample(frac=1).reset_index(drop=True)\n",
    "Labels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we do some pre-processing and cleaning for the text. \n",
    "1) loweing the  capital cases is the first one which is straightforward \n",
    "2) Stop words: words with high frequancy, and low frequency appreance (both are bad for the model as they have no information in them and just overfit the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    #text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "Labels['comment']=Labels.iloc[:,0].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels_train = Labels[0:2300]\n",
    "Labels_test = Labels[2300:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(Labels_train['comment'],  Labels_train['label'])\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(Labels_test['comment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Labels_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.39      0.66      0.49       152\n",
      "     neutral       0.38      0.36      0.37       173\n",
      "    positive       0.44      0.40      0.42       165\n",
      "     worried       1.00      0.03      0.05        80\n",
      "\n",
      "    accuracy                           0.41       570\n",
      "   macro avg       0.55      0.36      0.33       570\n",
      "weighted avg       0.49      0.41      0.37       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.44      0.44      0.44       152\n",
      "     neutral       0.42      0.37      0.40       173\n",
      "    positive       0.45      0.42      0.43       165\n",
      "     worried       0.25      0.36      0.30        80\n",
      "\n",
      "    accuracy                           0.40       570\n",
      "   macro avg       0.39      0.40      0.39       570\n",
      "weighted avg       0.41      0.40      0.41       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(Labels_train['comment'],  Labels_train['label'])\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(Labels_test['comment'])\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.43      0.48      0.46       152\n",
      "     neutral       0.40      0.43      0.41       173\n",
      "    positive       0.46      0.45      0.45       165\n",
      "     worried       0.33      0.23      0.27        80\n",
      "\n",
      "    accuracy                           0.42       570\n",
      "   macro avg       0.40      0.40      0.40       570\n",
      "weighted avg       0.42      0.42      0.42       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(Labels_train['comment'],  Labels_train['label'])\n",
    "\n",
    "\n",
    "y_pred = sgd.predict(Labels_test['comment'])\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(y_pred )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.columns = [\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels_test = Labels_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([Labels_test, pred], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
